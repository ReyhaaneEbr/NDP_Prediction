{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qiOcPIrAqYMZ",
        "jLCsl3wG3zKw",
        "cRrH1Je78VCt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MLP"
      ],
      "metadata": {
        "id": "upWsC3013MGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Libraries"
      ],
      "metadata": {
        "id": "qiOcPIrAqYMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------\n",
        "# Load Required Libraries\n",
        "# ---------------------------------------\n",
        "\n",
        "\n",
        "!pip install Bio"
      ],
      "metadata": {
        "id": "Z2e53OTAqU1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from Bio.Align import substitution_matrices\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from Bio.Align import substitution_matrices\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, classification_report\n",
        ")\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "MiEzhqlJcS_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLOSUM"
      ],
      "metadata": {
        "id": "3jjFe5oe3jj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_data = pd.read_csv('/data/seq_data.csv')\n",
        "blosum62 = substitution_matrices.load(\"BLOSUM62\")"
      ],
      "metadata": {
        "id": "ON7WtOOG3n7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim= 20\n",
        "drop_out=0.2\n",
        "\n",
        "NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(20,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ],
      "metadata": {
        "id": "nvyNoT8I5lqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm3Q9cV3bo3f"
      },
      "outputs": [],
      "source": [
        "# === Load manual folds ===\n",
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "        train_ids = pd.read_csv(train_path).iloc[:, 0]\n",
        "        test_ids = pd.read_csv(test_path).iloc[:, 0]\n",
        "        folds.append((train_ids, test_ids))\n",
        "    return folds\n",
        "\n",
        "# === BLOSUM62 feature extraction (20D mean pooling, standard AA) ===\n",
        "blosum62 = substitution_matrices.load(\"BLOSUM62\")\n",
        "aminos20 = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "aa_to_idx_20 = {aa: i for i, aa in enumerate(aminos20)}\n",
        "blosum_matrix_20 = np.zeros((20, 20))\n",
        "for i, aa1 in enumerate(aminos20):\n",
        "    for j, aa2 in enumerate(aminos20):\n",
        "        blosum_matrix_20[i, j] = blosum62[aa1, aa2]\n",
        "# Normalize\n",
        "mean, std = blosum_matrix_20.mean(), blosum_matrix_20.std()\n",
        "blosum_matrix_20 = (blosum_matrix_20 - mean) / std\n",
        "\n",
        "def seq_to_blosum_vector(seq):\n",
        "    vecs = []\n",
        "    for aa in seq.upper():\n",
        "        if aa in aa_to_idx_20:\n",
        "            idx = aa_to_idx_20[aa]\n",
        "            vecs.append(blosum_matrix_20[idx])\n",
        "        else:\n",
        "            vecs.append(np.zeros(20))\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(20)\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "\n",
        "\n",
        "data_path = \"/data/seq_data.csv\"\n",
        "folds_dir = \"/data/folds/5_folds\"\n",
        "#folds_dir = \"/data/folds/5_folds_IId\"\n",
        "df = pd.read_csv(data_path)\n",
        "folds = load_folds(folds_dir)\n",
        "\n",
        "df['blosum'] = df['seq'].apply(seq_to_blosum_vector)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"blosum\"] = df[\"blosum\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "blosum_cols = pd.DataFrame(df[\"blosum\"].tolist(), columns=[f\"blosum_{i+1}\" for i in range(20)])\n",
        "new_df = pd.concat([df.drop(columns=[\"blosum\"]), blosum_cols], axis=1)"
      ],
      "metadata": {
        "id": "VMiloldu5wwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNNTWCb5cjoL"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = new_df.drop(columns=['id', 'label','seq'])\n",
        "y = new_df['label']\n",
        "\n",
        "# Choose your balancing method: 'smote' or 'undersample'\n",
        "balancing = 'over'\n",
        "# balancing = 'undersample'\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    print(train_ids.shape)\n",
        "    print(test_ids.shape)\n",
        "    x_train = X[new_df['id'].isin(list(train_ids))]\n",
        "    x_test = X[new_df['id'].isin(list(test_ids))]\n",
        "    y_train = y[new_df['id'].isin(list(train_ids))]\n",
        "    y_test = y[new_df['id'].isin(list(test_ids))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dipeptide"
      ],
      "metadata": {
        "id": "0_uvacJe3ps8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nP2poK99SaS"
      },
      "outputs": [],
      "source": [
        "df_features = pd.read_csv('/data/embeddings/Dipeptide_embeddings.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GDrjh9gDI2w"
      },
      "outputs": [],
      "source": [
        "df_features.rename(columns={df_features.columns[0]: \"id\"}, inplace=True)\n",
        "new_df = df_features.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim= 400\n",
        "drop_out=0.2\n",
        "\n",
        "NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(400,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ],
      "metadata": {
        "id": "hs5Ya9tZNcP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "folds_dir = \"/data/folds/5_folds_IId\"\n",
        "#folds_dir = \"/data/folds/5_folds\"\n",
        "folds = load_folds(folds_dir)"
      ],
      "metadata": {
        "id": "hFkPIT5KJk6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqngcX2ZMRJ5"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = new_df.drop(columns=['id', 'label'])\n",
        "y = new_df['label']\n",
        "\n",
        "balancing = 'over'\n",
        "\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    print(train_ids.shape)\n",
        "    print(test_ids.shape)\n",
        "    x_train = X[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ProtBert"
      ],
      "metadata": {
        "id": "YSITZTxK3udp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbf5gli5VlX6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new_df = pd.read_csv('/data/embeddings/PotBert_embeddings.csv')\n",
        "new_df = new_df.iloc[:4819,:]\n",
        "new_df.rename(columns={new_df.columns[0]: \"id\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqgpu40R_9Yp"
      },
      "outputs": [],
      "source": [
        "input_dim=1024\n",
        "drop_out=0.2\n",
        "\n",
        "NN=tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(1024,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "folds_dir = \"/data/folds/5_folds_IId\"\n",
        "#folds_dir = \"/data/folds/5_folds\"\n",
        "folds = load_folds(folds_dir)"
      ],
      "metadata": {
        "id": "moXatJeLNV-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DFDquTu8ifQ"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = new_df.drop(columns=['id', 'label'])\n",
        "y = new_df['label']\n",
        "\n",
        "# Choose your balancing method: 'smote' or 'undersample'\n",
        "balancing = 'over'\n",
        "# balancing = 'undersample'\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    print(train_ids.shape)\n",
        "    print(test_ids.shape)\n",
        "    x_train = X[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ProtT5"
      ],
      "metadata": {
        "id": "jLCsl3wG3zKw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3h6jB2zVeZo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new_df = pd.read_csv('/data/embeddings/ProtT5_embeddings.csv')\n",
        "new_df = new_df.iloc[:4819,:]\n",
        "new_df.rename(columns={new_df.columns[0]: \"id\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL0BnV-NVeZo"
      },
      "outputs": [],
      "source": [
        "input_dim=1024\n",
        "drop_out=0.2\n",
        "\n",
        "NN=tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(1024,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "folds_dir = \"/data/folds/5_folds_IId\"\n",
        "#folds_dir = \"/data/folds/5_folds\"\n",
        "folds = load_folds(folds_dir)"
      ],
      "metadata": {
        "id": "E2rIAlb7VeZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbb-r5ttVeZp"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = new_df.drop(columns=['id', 'label'])\n",
        "y = new_df['label']\n",
        "\n",
        "# Choose your balancing method: 'smote' or 'undersample'\n",
        "balancing = 'over'\n",
        "# balancing = 'undersample'\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    print(train_ids.shape)\n",
        "    print(test_ids.shape)\n",
        "    x_train = X[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRrH1Je78VCt"
      },
      "source": [
        "##ESM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr5yW8lX8VCu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new_df = pd.read_csv('/data/embeddings/esm_embeddings.csv')\n",
        "new_df = new_df.iloc[:4819,:]\n",
        "new_df.rename(columns={new_df.columns[0]: \"id\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9a3UzAf8VCv"
      },
      "outputs": [],
      "source": [
        "input_dim= 320\n",
        "drop_out=0.2\n",
        "\n",
        "NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(320,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz0Tilfs8VCv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "folds_dir = \"/data/folds/5_folds_IId\"\n",
        "#folds_dir = \"/data/folds/5_folds\"\n",
        "folds = load_folds(folds_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TqC0HqB8VCw"
      },
      "outputs": [],
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = new_df.drop(columns=['id', 'label'])\n",
        "y = new_df['label']\n",
        "\n",
        "balancing = 'over'\n",
        "\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    print(train_ids.shape)\n",
        "    print(test_ids.shape)\n",
        "    x_train = X[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[new_df['id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[new_df['id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "JF639rcP3_dq"
      }
    }
  ]
}