{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TKqqI4PKbWIW",
        "cfY1bBUwAyA1",
        "rjcQmvol_9Yp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Word2vec_ProtT5"
      ],
      "metadata": {
        "id": "TKqqI4PKbWIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Libraries"
      ],
      "metadata": {
        "id": "cfY1bBUwAyA1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mG09vncAHLh"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Load Libraries\n",
        "# ===============================\n",
        "\n",
        "# Core\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Embeddings\n",
        "from gensim.models import Word2Vec\n",
        "import networkx as nx\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Class Imbalance\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, precision_recall_curve,\n",
        "    average_precision_score, auc\n",
        ")\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec"
      ],
      "metadata": {
        "id": "kQ08IYEsBezR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "KL9GjLOpHX9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_data = pd.read_csv('data/ppi_edges_iid.csv')\n",
        "df_features = pd.read_csv('data/protT5_embeddings.csv')\n",
        "\n",
        "df_feature = df_features.iloc[:4819, :]\n",
        "id_list = set(df_feature['Id'])\n",
        "G = nx.Graph()\n",
        "for id1, id2 in zip(graph_data.iloc[:, 0], graph_data.iloc[:, 1]):\n",
        "    if str(id1) in id_list and str(id2) in id_list:\n",
        "        G.add_edge(id1, id2)"
      ],
      "metadata": {
        "id": "LaaX2l17HcYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Word2Vec Embeddings"
      ],
      "metadata": {
        "id": "R7A3I73PIIKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "for i in range(len(graph_data)):\n",
        "    n1, n2 = graph_data.iloc[i, 0], graph_data.iloc[i, 1]\n",
        "    sentences.append(f\"{n1} is_connected {n2}\")\n",
        "\n",
        "for i in range(len(graph_data)):\n",
        "    n1, n2 = graph_data.iloc[i, 1], graph_data.iloc[i, 0]\n",
        "    sentences.append(f\"{n1} is_connected {n2}\")\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "nltk.download('punkt')\n",
        "tokenized_sentences = [word_tokenize(s.lower()) for s in sentences]\n",
        "\n",
        "\n",
        "# Train Word2Vec Model\n",
        "w2v_model = Word2Vec(\n",
        "    min_count=1,\n",
        "    alpha=0.001,\n",
        "    vector_size=512,\n",
        "    window=2,\n",
        "    epochs=1000,\n",
        "    sg=1\n",
        ")\n",
        "\n",
        "w2v_model.build_vocab(tokenized_sentences)\n",
        "w2v_model.train(\n",
        "    tokenized_sentences,\n",
        "    total_examples=w2v_model.corpus_count,\n",
        "    epochs=w2v_model.epochs\n",
        ")\n",
        "\n",
        "\n",
        "# Extract Node Embeddings\n",
        "vocab = w2v_model.wv.index_to_key\n",
        "\n",
        "node_vectors = {\n",
        "    str(node): w2v_model.wv[str(node)]\n",
        "    for node in vocab\n",
        "}\n",
        "\n",
        "node_vector = (\n",
        "    pd.DataFrame.from_dict(node_vectors, orient='index')\n",
        "    .reset_index()\n",
        "    .rename(columns={'index': 'Id'})\n",
        ")\n",
        "\n",
        "node_vector.to_csv(\"word2vec_features.csv\", index=False)\n",
        "\n",
        "\n",
        "## Merge Word2Vec with Labels\n",
        "df_features = df_features.rename(columns={df_features.columns[0]: 'Id'})\n",
        "\n",
        "filtered_node_vector = node_vector[\n",
        "    node_vector['Id'].isin(df_features['Id'])\n",
        "].copy()\n",
        "\n",
        "word2vec_vec = pd.merge(\n",
        "    filtered_node_vector,\n",
        "    df_features[['Id', 'label']],\n",
        "    on='Id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "word2vec_vec = word2vec_vec[word2vec_vec['label'] != -1]"
      ],
      "metadata": {
        "id": "pX-ejPsUFDYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine with Prot T5 Features"
      ],
      "metadata": {
        "id": "LvEKBL_TLQe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_add = [c for c in df_features.columns if c != 'label']\n",
        "\n",
        "merged_df = pd.merge(\n",
        "    word2vec_vec,\n",
        "    df_features[cols_to_add],\n",
        "    on='Id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "cols = [c for c in merged_df.columns if c != 'label'] + ['label']\n",
        "merged_df = merged_df[cols]\n",
        "\n",
        "\n",
        "##Rename Feature Columns and Save\n",
        "num_features = 1536\n",
        "feature_names = [f'v{i}' for i in range(1, num_features + 1)]\n",
        "merged_df.columns = ['Id'] + feature_names + ['label']"
      ],
      "metadata": {
        "id": "SJJnXqbILT03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-4MpR0b5q-u"
      },
      "source": [
        "##Neural Network Word2Vec\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPz9iB0q5q-u"
      },
      "outputs": [],
      "source": [
        "input_dim=512\n",
        "drop_out=0.5\n",
        "\n",
        "NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPHa2rGH5q-v"
      },
      "outputs": [],
      "source": [
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "# Example usage\n",
        "folds_dir = \"/data/5folds_IId\"  # Update this path to your Google Drive directory\n",
        "folds = load_folds(folds_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = word2vec_vec.drop(columns=['Id', 'label'])\n",
        "y = word2vec_vec['label']\n",
        "\n",
        "# Choose your balancing method: 'smote' or 'undersample'\n",
        "balancing = 'over'\n",
        "# balancing = 'undersample'\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    reset_weights(NN, initial_weights)\n",
        "    x_train = X[word2vec_vec['Id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[word2vec_vec['Id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[word2vec_vec['Id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[word2vec_vec['Id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, Avg Precision: {avg_precision:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ],
      "metadata": {
        "id": "cPm49W_P5q-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjcQmvol_9Yp"
      },
      "source": [
        "##Neural Network Concat (Prot T5,Word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqgpu40R_9Yp"
      },
      "outputs": [],
      "source": [
        "input_dim= 1536\n",
        "drop_out=0.5\n",
        "\n",
        "NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(1536,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(128,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(16,input_dim=input_dim, activation='relu'),\n",
        "    tf.keras.layers.Dropout(drop_out),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "NN.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5XfdaA_lfDj"
      },
      "outputs": [],
      "source": [
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "        folds.append((train_ids, test_ids))\n",
        "\n",
        "    return folds\n",
        "\n",
        "def store_initial_weights(model):\n",
        "    # Store the initial weights\n",
        "    initial_weights = model.get_weights()\n",
        "    return initial_weights\n",
        "\n",
        "def reset_weights(model, initial_weights):\n",
        "    # Reset the weights to the stored initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "\n",
        "initial_weights = store_initial_weights(NN)\n",
        "# Example usage\n",
        "folds_dir = \"/data/5folds_IId\"  # Update this path to your Google Drive directory\n",
        "folds = load_folds(folds_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for storing fold metrics\n",
        "accuracy_list, precision_list, recall_list = [], [], []\n",
        "f1score_list, auc_list, avg_precision_list, auprc_list = [], [], [], []\n",
        "\n",
        "X = merged_df.drop(columns=['Id', 'label'])\n",
        "y = merged_df['label']\n",
        "\n",
        "# Choose your balancing method: 'smote' or 'undersample'\n",
        "balancing = 'over'\n",
        "# balancing = 'undersample'\n",
        "\n",
        "for fold_num, (train_ids, test_ids) in enumerate(folds, 1):\n",
        "\n",
        "    # Assuming reset_weights and NN, initial_weights are defined elsewhere\n",
        "    # reset_weights(NN, initial_weights)\n",
        "\n",
        "    # Split\n",
        "    reset_weights(NN, initial_weights)\n",
        "    x_train = X[merged_df['Id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    x_test = X[merged_df['Id'].isin(list(test_ids.iloc[:,0]))]\n",
        "    y_train = y[merged_df['Id'].isin(list(train_ids.iloc[:,0]))]\n",
        "    y_test = y[merged_df['Id'].isin(list(test_ids.iloc[:,0]))]\n",
        "\n",
        "    # --- Data Balancing on Train only ---\n",
        "    if balancing == 'over':\n",
        "        #sm = SMOTE(random_state=42)\n",
        "        #x_train_bal, y_train_bal = sm.fit_resample(x_train, y_train)\n",
        "        oversampler = RandomOverSampler(random_state=42)\n",
        "        # Apply oversampling\n",
        "        x_train_bal, y_train_bal = oversampler.fit_resample(x_train, y_train) # <--- CORRECTED HERE: y_train_balr to y_train_bal\n",
        "    elif balancing == 'undersample':\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        x_train_bal, y_train_bal = rus.fit_resample(x_train, y_train)\n",
        "    else:\n",
        "        x_train_bal, y_train_bal = x_train, y_train\n",
        "\n",
        "    # --- Train on balanced data ---\n",
        "    # The NN.fit call now correctly uses y_train_bal\n",
        "    NN.fit(x_train_bal, y_train_bal, epochs=50, batch_size=32)\n",
        "    pred = NN.predict(x_test)\n",
        "\n",
        "    # Evaluate\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (pred >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    precision = precision_score(y_test, binary_predictions)\n",
        "    recall = recall_score(y_test, binary_predictions)\n",
        "    f1 = f1_score(y_test, binary_predictions)\n",
        "    auc_score = roc_auc_score(y_test, pred)\n",
        "    precision_values, recall_values, _ = precision_recall_curve(y_test, pred)\n",
        "    avg_precision = average_precision_score(y_test, pred)\n",
        "    auprc = auc(recall_values, precision_values)\n",
        "\n",
        "    # Store\n",
        "    accuracy_list.append(accuracy)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1score_list.append(f1)\n",
        "    auc_list.append(auc_score)\n",
        "    avg_precision_list.append(avg_precision)\n",
        "    auprc_list.append(auprc)\n",
        "\n",
        "    print(f\"Fold {fold_num} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, \"\n",
        "          f\"AUC: {auc_score:.4f}, Avg Precision: {avg_precision:.4f}, AUPRC: {auprc:.4f}\")\n",
        "\n",
        "def print_mean_std(metric_list, metric_name):\n",
        "    print(f\"{metric_name:<15}: {np.mean(metric_list):.4f} ± {np.std(metric_list):.4f}\")\n",
        "\n",
        "print(\"\\nAverage metrics across all folds (mean ± std):\")\n",
        "print_mean_std(accuracy_list, \"Accuracy\")\n",
        "print_mean_std(precision_list, \"Precision\")\n",
        "print_mean_std(recall_list, \"Recall\")\n",
        "print_mean_std(f1score_list, \"F1 Score\")\n",
        "print_mean_std(auc_list, \"AUC\")\n",
        "print_mean_std(auprc_list, \"AUPRC\")"
      ],
      "metadata": {
        "id": "1DFDquTu8ifQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}